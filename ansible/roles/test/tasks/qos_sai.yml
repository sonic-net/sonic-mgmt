# To run ecn test the host system where ptf container resides should have
# optimized sysctl parameter "net.core.rmem_max". Now it's set to 4194304
# Also the NICs supposed to have maximum buffer size of RX queue
# See: ethtool -g
#      ethtool -G p4p1 rx 8192

- include_vars: vars/qos.yml

- set_fact:
    disable_test: "{{disable_test | default('true') | bool}}"

- include_tasks: add_container_to_inventory.yml
  vars:
    container_name: "{{ item }}"
  with_items:
    - "lldp"
    - "bgp"
    - "syncd"

- block:
    - name: Getting minigraph facts
      minigraph_facts: host={{inventory_hostname}}
      become: no

    - name: Get ports info.
      include_tasks: roles/test/tasks/qos_get_ports.yml

    - name: Check if lossless buffer profile is derived
      fail: msg="Lossless Buffer profile could not be retreived"
      when: lossless_buffer_profile is not defined or minigraph_hwsku is not defined

    - set_fact:
        defined_asic_list: ['td2', 'th', 'th2', 'spc1', 'spc2', 'spc3']
        speed_cablelen: "{{ lossless_buffer_profile }}"

    - name: gather sonic release if not available
      sonic_release:
      when: sonic_release is not defined

    - fail:
        msg: "Unable to get sonic release and qos db format information"
      when: sonic_qos_db_fv_reference_with_table is not defined

    - name: Set speed_cablelen with new db format
      set_fact: speed_cablelen="{{speed_cablelen | regex_replace('pg_lossless_(.*)_profile', '\\1')}}"
      when: sonic_qos_db_fv_reference_with_table|bool == false
    
    - name: Set speed_cablelen with old db format
      set_fact: speed_cablelen="{{speed_cablelen | regex_replace('BUFFER_PROFILE\|pg_lossless_(.*)_profile', '\\1')}}"
      when: sonic_qos_db_fv_reference_with_table|bool == true

    - name: Get asic type
      set_fact: asic_type="{{ item }}"
      when:
        - hostvars[inventory_hostname][sonic_asic_type + '_' + item + '_hwskus'] is defined
        - minigraph_hwsku in hostvars[inventory_hostname][sonic_asic_type + '_' + item + '_hwskus']
      with_items: "{{ defined_asic_list }}"

    - debug: msg="asic type is {{ asic_type }}, portspeed_cablelen is {{ speed_cablelen }}"

    - name: check if the device has configured qos parameters
      fail: msg="device doesn't have configured qos parameters"
      when: qos_params[asic_type] is not defined or qos_params[asic_type][speed_cablelen] is not defined

    - name: set qos parameters for the device
      set_fact:
        qp: "{{qos_params[asic_type]}}"
        qp_sc: "{{qos_params[asic_type][speed_cablelen]}}"

    - name: Ensure LLDP Daemon stopped
      become: yes
      supervisorctl: state=stopped name={{item}}
      delegate_to: "{{ ansible_host }}_lldp"
      with_items:
        - lldpd
        - lldp-syncd

    - name: Ensure BGP Daemon stopped
      become: yes
      supervisorctl: state=stopped name=bgpd
      delegate_to: "{{ ansible_host }}_bgp"
    
    - name: Add iptables rule to drop BGP SYN Packet from peer so that we do not ACK back. Add at top so existing rules don't have precedence over it.
      shell: "iptables -I INPUT 1 -j DROP -p tcp --destination-port bgp"
      become: true

    - name: Add ip6tables rule to drop BGP SYN Packet from peer so that we do not ACK back. Add at top so existing rules don't have precedence over it.
      shell: "ip6tables -I INPUT 1 -j DROP -p tcp --destination-port bgp"
      become: true

    - meta: flush_handlers

    - name: Deploy script to DUT/syncd
      copy: src=roles/test/files/mlnx/packets_aging.py dest=/root/packets_aging.py
      delegate_to: "{{ ansible_host }}_syncd"
      when: minigraph_hwsku is defined and minigraph_hwsku in mellanox_hwskus

    - name: Disable Mellanox packet aging
      shell: python /root/packets_aging.py disable
      register: result
      failed_when: result.stderr != ''
      delegate_to: "{{ ansible_host }}_syncd"
      when: minigraph_hwsku is defined and minigraph_hwsku in mellanox_hwskus

    - name: copy ptf tests
      copy: src=roles/test/files/ptftests dest=/root
      delegate_to: "{{ptf_host}}"

    - name: copy sai tests
      copy: src=roles/test/files/saitests dest=/root
      delegate_to: "{{ptf_host}}"

    - name: copy portmap
      copy: src={{ptf_portmap}} dest=/root
      delegate_to: "{{ptf_host}}"

    - name: Init PTF base test parameters
      set_fact:
        ptf_base_params:
        - router_mac={% if testbed_type not in ['t0', 't0-64', 't0-116'] %}'{{ansible_Ethernet0['macaddress']}}'{% else %}''{% endif %}
        - server='{{ansible_host}}'
        - port_map_file='/root/{{ptf_portmap | basename}}'
        - sonic_asic_type='{{sonic_asic_type}}'

    # Unpause all paused port
    - include_tasks: qos_sai_ptf.yml
      vars:
        test_name: release all paused ports
        test_path: sai_qos_tests.ReleaseAllPorts
        test_params: []

    # Populate arps
    - name: Check if DUT has ARP aging issue or not
      command: arp -n
      become: yes
      register: arp_entries

    - debug:
        var: arp_entries

    - include_tasks: qos_sai_ptf.yml
      vars:
        test_name: populate arp on all ports
        test_path: sai_qos_tests.ARPpopulate
        test_params:
        - dst_port_id='{{dst_port_id}}'
        - dst_port_ip='{{dst_port_ip}}'
        - dst_port_2_id='{{dst_port_2_id}}'
        - dst_port_2_ip='{{dst_port_2_ip}}'
        - dst_port_3_id='{{dst_port_3_id}}'
        - dst_port_3_ip='{{dst_port_3_ip}}'
        - src_port_id='{{src_port_id}}'
        - src_port_ip='{{src_port_ip}}'
      when: testbed_type in ['t0', 't0-64', 't0-116'] or arp_entries.stdout.find('incomplete') == -1

    - include_tasks: qos_sai_ptf.yml
      vars:
        test_name: populate arp on all ports
        test_path: sai_qos_tests.ARPpopulatePTF
        test_params:
        - dst_port_id='{{dst_port_id}}'
        - dst_port_ip='{{dst_port_ip}}'
        - dst_port_2_id='{{dst_port_2_id}}'
        - dst_port_2_ip='{{dst_port_2_ip}}'
        - dst_port_3_id='{{dst_port_3_id}}'
        - dst_port_3_ip='{{dst_port_3_ip}}'
        - src_port_id='{{src_port_id}}'
        - src_port_ip='{{src_port_ip}}'
      when: testbed_type in ['ptf32', 'ptf64']

    # XOFF limit
    - include_tasks: qos_sai_ptf.yml
      vars:
        test_name: xoff limit ptf test dscp = {{qp_sc.xoff_1.dscp}}, ecn = {{qp_sc.xoff_1.ecn}}
        test_path: sai_qos_tests.PFCtest
        test_params:
        - dscp='{{qp_sc.xoff_1.dscp}}'
        - ecn='{{qp_sc.xoff_1.ecn}}'
        - pg='{{qp_sc.xoff_1.pg}}'
        - buffer_max_size='{{lossless_buffer_max_size|int}}'
        - queue_max_size='{{lossless_queue_max_size|int}}'
        - dst_port_id='{{dst_port_id}}'
        - dst_port_ip='{{dst_port_ip}}'
        - src_port_id='{{src_port_id}}'
        - src_port_ip='{{src_port_ip}}'
        - pkts_num_leak_out='{{qp_sc.pkts_num_leak_out}}'
        - pkts_num_trig_pfc='{{qp_sc.xoff_1.pkts_num_trig_pfc}}'
        - pkts_num_trig_ingr_drp='{{qp_sc.xoff_1.pkts_num_trig_ingr_drp}}'

    - include_tasks: qos_sai_ptf.yml
      vars:
        test_name: xoff limit ptf test dscp = {{qp_sc.xoff_2.dscp}}, ecn = {{qp_sc.xoff_2.ecn}}
        test_path: sai_qos_tests.PFCtest
        test_params:
        - dscp='{{qp_sc.xoff_2.dscp}}'
        - ecn='{{qp_sc.xoff_2.ecn}}'
        - pg='{{qp_sc.xoff_2.pg}}'
        - buffer_max_size='{{lossless_buffer_max_size|int}}'
        - queue_max_size='{{lossless_queue_max_size|int}}'
        - dst_port_id='{{dst_port_id}}'
        - dst_port_ip='{{dst_port_ip}}'
        - src_port_id='{{src_port_id}}'
        - src_port_ip='{{src_port_ip}}'
        - pkts_num_leak_out='{{qp_sc.pkts_num_leak_out}}'
        - pkts_num_trig_pfc='{{qp_sc.xoff_2.pkts_num_trig_pfc}}'
        - pkts_num_trig_ingr_drp='{{qp_sc.xoff_2.pkts_num_trig_ingr_drp}}'

    # XON limit
    - include_tasks: qos_sai_ptf.yml
      vars:
        test_name: xon limit ptf test dscp = {{qp.xon_1.dscp}}, ecn = {{qp.xon_1.ecn}}
        test_path: sai_qos_tests.PFCXonTest
        test_params:
        - dscp='{{qp.xon_1.dscp}}'
        - ecn='{{qp.xon_1.ecn}}'
        - pg='{{qp.xon_1.pg}}'
        - buffer_max_size='{{lossless_buffer_max_size|int}}'
        - dst_port_id='{{dst_port_id}}'
        - dst_port_ip='{{dst_port_ip}}'
        - dst_port_2_id='{{dst_port_2_id}}'
        - dst_port_2_ip='{{dst_port_2_ip}}'
        - dst_port_3_id='{{dst_port_3_id}}'
        - dst_port_3_ip='{{dst_port_3_ip}}'
        - src_port_id='{{src_port_id}}'
        - src_port_ip='{{src_port_ip}}'
        - pkts_num_leak_out='{{qp_sc.pkts_num_leak_out}}'
        - pkts_num_trig_pfc='{{qp.xon_1.pkts_num_trig_pfc}}'
        - pkts_num_dismiss_pfc='{{qp.xon_1.pkts_num_dismiss_pfc}}'

    - include_tasks: qos_sai_ptf.yml
      vars:
        test_name: xon limit ptf test dscp = {{qp.xon_2.dscp}}, ecn = {{qp.xon_2.ecn}}
        test_path: sai_qos_tests.PFCXonTest
        test_params:
        - dscp='{{qp.xon_2.dscp}}'
        - ecn='{{qp.xon_2.ecn}}'
        - pg='{{qp.xon_2.pg}}'
        - buffer_max_size='{{lossless_buffer_max_size|int}}'
        - dst_port_id='{{dst_port_id}}'
        - dst_port_ip='{{dst_port_ip}}'
        - dst_port_2_id='{{dst_port_2_id}}'
        - dst_port_2_ip='{{dst_port_2_ip}}'
        - dst_port_3_id='{{dst_port_3_id}}'
        - dst_port_3_ip='{{dst_port_3_ip}}'
        - src_port_id='{{src_port_id}}'
        - src_port_ip='{{src_port_ip}}'
        - pkts_num_leak_out='{{qp_sc.pkts_num_leak_out}}'
        - pkts_num_trig_pfc='{{qp.xon_2.pkts_num_trig_pfc}}'
        - pkts_num_dismiss_pfc='{{qp.xon_2.pkts_num_dismiss_pfc}}'

    # Headroom pool size
    - include_tasks: qos_sai_ptf.yml
      vars:
        test_name: headroom pool size ptf test ecn = {{qp_sc.hdrm_pool_size.ecn}}
        test_path: sai_qos_tests.HdrmPoolSizeTest
        test_params:
        - testbed_type='{{testbed_type}}'
        - dscps={{qp_sc.hdrm_pool_size.dscps}}
        - ecn={{qp_sc.hdrm_pool_size.ecn}}
        - pgs={{qp_sc.hdrm_pool_size.pgs}}
        - src_port_ids={{qp_sc.hdrm_pool_size.src_port_ids}}
        - src_port_ips=[{% for pid in qp_sc.hdrm_pool_size.src_port_ids %}{% if not loop.last %}'{{testing_ports_ip[pid|string]}}', {% else %}'{{testing_ports_ip[pid|string]}}'{% endif %}{% endfor %}]
        - dst_port_id={{qp_sc.hdrm_pool_size.dst_port_id}}
        - dst_port_ip='{{testing_ports_ip[qp_sc.hdrm_pool_size.dst_port_id|string]}}'
        - pgs_num={{qp_sc.hdrm_pool_size.pgs_num }}
        - pkts_num_leak_out={{qp_sc.pkts_num_leak_out}}
        - pkts_num_trig_pfc={{qp_sc.hdrm_pool_size.pkts_num_trig_pfc}}
        - pkts_num_hdrm_full={{qp_sc.hdrm_pool_size.pkts_num_hdrm_full}}
        - pkts_num_hdrm_partial={{qp_sc.hdrm_pool_size.pkts_num_hdrm_partial}}
      when: minigraph_hwsku is defined and
            minigraph_hwsku in ['Arista-7060CX-32S-C32', 'Celestica-DX010-C32', 'Arista-7260CX3-D108C8', 'Force10-S6100', 'Arista-7260CX3-Q64']

    # Lossy queue
    - include_tasks: qos_sai_ptf.yml
      vars:
        test_name: Lossy queue, shared buffer dynamic allocation. dscp = {{qp.lossy_queue_1.dscp}}, ecn = {{qp.lossy_queue_1.ecn}}
        test_path: sai_qos_tests.LossyQueueTest
        test_params:
        - dscp='{{qp.lossy_queue_1.dscp}}'
        - ecn='{{qp.lossy_queue_1.ecn}}'
        - pg='{{qp.lossy_queue_1.pg}}'
        - buffer_max_size='{{lossy_buffer_max_size|int}}'
        - headroom_size='{{lossy_headroom_size}}'
        - dst_port_id='{{dst_port_id}}'
        - dst_port_ip='{{dst_port_ip}}'
        - dst_port_2_id='{{dst_port_2_id}}'
        - dst_port_2_ip='{{dst_port_2_ip}}'
        - src_port_id='{{src_port_id}}'
        - src_port_ip='{{src_port_ip}}'
        - pkts_num_leak_out='{{qp_sc.pkts_num_leak_out}}'
        - pkts_num_trig_egr_drp='{{qp.lossy_queue_1.pkts_num_trig_egr_drp}}'

    # DSCP to queue mapping
    - include_tasks: qos_sai_ptf.yml
      vars:
        test_name: dscp to queue mapping ptf test
        test_path: sai_qos_tests.DscpMappingPB
        test_params:
        - dst_port_id='{{dst_port_id}}'
        - dst_port_ip='{{dst_port_ip}}'
        - src_port_id='{{src_port_id}}'
        - src_port_ip='{{src_port_ip}}'

    # WRR test
    - include_tasks: qos_sai_ptf.yml
      vars:
        test_name: DWRR
        test_path: sai_qos_tests.WRRtest
        test_params:
        - ecn='{{qp.wrr.ecn}}'
        - dst_port_id='{{dst_port_id}}'
        - dst_port_ip='{{dst_port_ip}}'
        - src_port_id='{{src_port_id}}'
        - src_port_ip='{{src_port_ip}}'
        - q0_num_of_pkts='{{qp.wrr.q0_num_of_pkts}}'
        - q1_num_of_pkts='{{qp.wrr.q1_num_of_pkts}}'
        - q2_num_of_pkts='{{qp.wrr.q2_num_of_pkts}}'
        - q3_num_of_pkts='{{qp.wrr.q3_num_of_pkts}}'
        - q4_num_of_pkts='{{qp.wrr.q4_num_of_pkts}}'
        - q5_num_of_pkts='{{qp.wrr.q5_num_of_pkts}}'
        - q6_num_of_pkts='{{qp.wrr.q6_num_of_pkts}}'
        - limit='{{qp.wrr.limit}}'
        - pkts_num_leak_out='{{qp_sc.pkts_num_leak_out}}'
    - debug:
        var: out.stdout_lines

    # Clear all watermarks before each watermark test
    # because of the clear on read polling mode
    - name: Toggle watermark polling
      shell: bash -c 'counterpoll watermark enable; sleep 20; counterpoll watermark disable'

    # PG shared watermark test
    - include_tasks: qos_sai_ptf.yml
      vars:
        test_name: PG shared watermark test, lossless traffic
        test_path: sai_qos_tests.PGSharedWatermarkTest
        test_params:
        - dscp='{{qp.wm_pg_shared_lossless.dscp}}'
        - ecn='{{qp.wm_pg_shared_lossless.ecn}}'
        - pg='{{qp.wm_pg_shared_lossless.pg}}'
        - dst_port_id='{{dst_port_id}}'
        - dst_port_ip='{{dst_port_ip}}'
        - src_port_id='{{src_port_id}}'
        - src_port_ip='{{src_port_ip}}'
        - pkts_num_leak_out='{{qp_sc.pkts_num_leak_out}}'
        - pkts_num_fill_min='{{qp.wm_pg_shared_lossless.pkts_num_fill_min}}'
        - pkts_num_fill_shared='{{qp.wm_pg_shared_lossless.pkts_num_trig_pfc}}'
        - packet_size='{{qp.wm_pg_shared_lossless.packet_size}}'
        - cell_size='{{qp.wm_pg_shared_lossless.cell_size}}'
      when: minigraph_hwsku is defined and
            (minigraph_hwsku not in ['Arista-7260CX3-Q64', 'Arista-7260CX3-D108C8'])
    - debug:
        var: out.stdout_lines
      when: minigraph_hwsku is defined and
            (minigraph_hwsku not in ['Arista-7260CX3-Q64', 'Arista-7260CX3-D108C8'])

    # Clear all watermarks before each watermark test
    # because of the clear on read polling mode
    - name: Toggle watermark polling
      shell: bash -c 'counterpoll watermark enable; sleep 20; counterpoll watermark disable'

    # PG shared watermark test
    - include_tasks: qos_sai_ptf.yml
      vars:
        test_name: PG shared watermark test, lossy traffic
        test_path: sai_qos_tests.PGSharedWatermarkTest
        test_params:
        - dscp='{{qp.wm_pg_shared_lossy.dscp}}'
        - ecn='{{qp.wm_pg_shared_lossy.ecn}}'
        - pg='{{qp.wm_pg_shared_lossy.pg}}'
        - dst_port_id='{{dst_port_id}}'
        - dst_port_ip='{{dst_port_ip}}'
        - src_port_id='{{src_port_id}}'
        - src_port_ip='{{src_port_ip}}'
        - pkts_num_leak_out='{{qp_sc.pkts_num_leak_out}}'
        - pkts_num_fill_min='{{qp.wm_pg_shared_lossy.pkts_num_fill_min}}'
        - pkts_num_fill_shared='{{qp.wm_pg_shared_lossy.pkts_num_trig_egr_drp|int - 1}}'
        - packet_size='{{qp.wm_pg_shared_lossy.packet_size}}'
        - cell_size='{{qp.wm_pg_shared_lossy.cell_size}}'
      when: minigraph_hwsku is defined and
            minigraph_hwsku not in ['Arista-7260CX3-Q64', 'Arista-7260CX3-D108C8']
    - debug:
        var: out.stdout_lines
      when: minigraph_hwsku is defined and
            minigraph_hwsku not in ['Arista-7260CX3-Q64', 'Arista-7260CX3-D108C8']

    # Clear all watermarks before each watermark test
    # because of the clear on read polling mode
    - name: Toggle watermark polling
      shell: bash -c 'counterpoll watermark enable; sleep 20; counterpoll watermark disable'

    # PG headroom watermark test
    - include_tasks: qos_sai_ptf.yml
      vars:
        test_name: PG headroom watermark test
        test_path: sai_qos_tests.PGHeadroomWatermarkTest
        test_params:
        - dscp='{{qp_sc.wm_pg_headroom.dscp}}'
        - ecn='{{qp_sc.wm_pg_headroom.ecn}}'
        - pg='{{qp_sc.wm_pg_headroom.pg}}'
        - dst_port_id='{{dst_port_id}}'
        - dst_port_ip='{{dst_port_ip}}'
        - src_port_id='{{src_port_id}}'
        - src_port_ip='{{src_port_ip}}'
        - pkts_num_leak_out='{{qp_sc.pkts_num_leak_out}}'
        - pkts_num_trig_pfc='{{qp_sc.wm_pg_headroom.pkts_num_trig_pfc}}'
        - pkts_num_trig_ingr_drp='{{qp_sc.wm_pg_headroom.pkts_num_trig_ingr_drp}}'
        - cell_size='{{qp_sc.wm_pg_headroom.cell_size}}'
    - debug:
        var: out.stdout_lines

    # Clear all watermarks before each watermark test
    # because of the clear on read polling mode
    - name: Toggle watermark polling
      shell: bash -c 'counterpoll watermark enable; sleep 20; counterpoll watermark disable'

    # Queue shared watermark test
    - include_tasks: qos_sai_ptf.yml
      vars:
        test_name: Queue shared watermark test, lossless traffic
        test_path: sai_qos_tests.QSharedWatermarkTest
        test_params:
        - dscp='{{qp_sc.wm_q_shared_lossless.dscp}}'
        - ecn='{{qp_sc.wm_q_shared_lossless.ecn}}'
        - queue='{{qp_sc.wm_q_shared_lossless.queue}}'
        - dst_port_id='{{dst_port_id}}'
        - dst_port_ip='{{dst_port_ip}}'
        - src_port_id='{{src_port_id}}'
        - src_port_ip='{{src_port_ip}}'
        - pkts_num_leak_out='{{qp_sc.pkts_num_leak_out}}'
        - pkts_num_fill_min='{{qp_sc.wm_q_shared_lossless.pkts_num_fill_min}}'
        - pkts_num_trig_drp='{{qp_sc.wm_q_shared_lossless.pkts_num_trig_ingr_drp}}'
        - cell_size='{{qp_sc.wm_q_shared_lossless.cell_size}}'
    - debug:
        var: out.stdout_lines

    # Clear all watermarks before each watermark test
    # because of the clear on read polling mode
    - name: Toggle watermark polling
      shell: bash -c 'counterpoll watermark enable; sleep 20; counterpoll watermark disable'

    # Queue shared watermark test
    - include_tasks: qos_sai_ptf.yml
      vars:
        test_name: Queue shared watermark test, lossy traffic
        test_path: sai_qos_tests.QSharedWatermarkTest
        test_params:
        - dscp='{{qp.wm_q_shared_lossy.dscp}}'
        - ecn='{{qp.wm_q_shared_lossy.ecn}}'
        - queue='{{qp.wm_q_shared_lossy.queue}}'
        - dst_port_id='{{dst_port_id}}'
        - dst_port_ip='{{dst_port_ip}}'
        - src_port_id='{{src_port_id}}'
        - src_port_ip='{{src_port_ip}}'
        - pkts_num_leak_out='{{qp_sc.pkts_num_leak_out}}'
        - pkts_num_fill_min='{{qp.wm_q_shared_lossy.pkts_num_fill_min}}'
        - pkts_num_trig_drp='{{qp.wm_q_shared_lossy.pkts_num_trig_egr_drp}}'
        - cell_size='{{qp.wm_q_shared_lossy.cell_size}}'
    - debug:
        var: out.stdout_lines

    - include: buff_wm.yml
      when: not disable_test

    # DSCP to pg mapping
    - include_tasks: qos_sai_ptf.yml
      vars:
        test_name: dscp to pg mapping ptf test
        test_path: sai_qos_tests.DscpToPgMapping
        test_params:
        - dst_port_id='{{dst_port_id}}'
        - dst_port_ip='{{dst_port_ip}}'
        - src_port_id='{{src_port_id}}'
        - src_port_ip='{{src_port_ip}}'
      when: not disable_test

    - debug:
        var: out.stdout_lines
      when: not disable_test

    # Change lossy and lossless scheduler weights
    - name: Change lossy scheduler weight to {{qp.wrr_chg.lossy_weight}}
      command: redis-cli -n 4 HSET "{{lossy_sched_profile}}" weight {{qp.wrr_chg.lossy_weight}}

    - name: Change lossless scheduler weight to {{qp.wrr_chg.lossless_weight}}
      command: redis-cli -n 4 HSET "{{lossless_sched_profile}}" weight {{qp.wrr_chg.lossless_weight}}

    # WRR test
    - include_tasks: qos_sai_ptf.yml
      vars:
        test_name: DWRR runtime weight change
        test_path: sai_qos_tests.WRRtest
        test_params:
        - ecn='{{qp.wrr_chg.ecn}}'
        - dst_port_id='{{dst_port_id}}'
        - dst_port_ip='{{dst_port_ip}}'
        - src_port_id='{{src_port_id}}'
        - src_port_ip='{{src_port_ip}}'
        - q0_num_of_pkts='{{qp.wrr_chg.q0_num_of_pkts}}'
        - q1_num_of_pkts='{{qp.wrr_chg.q1_num_of_pkts}}'
        - q2_num_of_pkts='{{qp.wrr_chg.q2_num_of_pkts}}'
        - q3_num_of_pkts='{{qp.wrr_chg.q3_num_of_pkts}}'
        - q4_num_of_pkts='{{qp.wrr_chg.q4_num_of_pkts}}'
        - q5_num_of_pkts='{{qp.wrr_chg.q5_num_of_pkts}}'
        - q6_num_of_pkts='{{qp.wrr_chg.q6_num_of_pkts}}'
        - limit='{{qp.wrr_chg.limit}}'
        - pkts_num_leak_out='{{qp_sc.pkts_num_leak_out}}'
    - debug:
        var: out.stdout_lines

    # Restore lossy and lossless scheduler weights
    - name: Restore lossy scheduler weight to {{lossy_sched_weight}}
      command: redis-cli -n 4 HSET "{{lossy_sched_profile}}" weight "{{lossy_sched_weight.stdout}}"

    - name: Restore lossless scheduler weight to {{lossless_sched_weight}}
      command: redis-cli -n 4 HSET "{{lossless_sched_profile}}" weight "{{lossless_sched_weight.stdout}}"

  always:
    - name: Restore LLDP Daemon
      become: yes
      supervisorctl: state=started name={{item}}
      delegate_to: "{{ ansible_host }}_lldp"
      with_items:
        - lldpd
        - lldp-syncd

    - name: Remove iptables rule to drop BGP SYN Packet from Peer
      shell: "iptables -D INPUT -j DROP -p tcp --destination-port bgp"
      become: true

    - name: Remove ip6tables rule to drop BGP SYN Packet from Peer
      shell: "ip6tables -D INPUT -j DROP -p tcp --destination-port bgp"
      become: true

    - name: Enable bgpd
      become: yes
      supervisorctl: state=started name=bgpd
      delegate_to: "{{ ansible_host }}_bgp"
      notify:
        - Restart Quagga Daemon

    - name: Restore original watermark polling status
      shell: counterpoll watermark {{watermark_status.stdout}}
      when:  watermark_status is defined and (watermark_status.stdout == "enable" or watermark_status.stdout == "disable")

    - name: Restore lossy scheduler weight to {{lossy_sched_weight}}
      command: redis-cli -n 4 HSET "{{lossy_sched_profile}}" weight "{{lossy_sched_weight.stdout}}"

    - name: Restore lossless scheduler weight to {{lossless_sched_weight}}
      command: redis-cli -n 4 HSET "{{lossless_sched_profile}}" weight "{{lossless_sched_weight.stdout}}"

    - name: Enable Mellanox packet aging
      shell: python /root/packets_aging.py enable
      register: result
      failed_when: result.stderr != ''
      delegate_to: "{{ ansible_host }}_syncd"
      when: minigraph_hwsku is defined and minigraph_hwsku in mellanox_hwskus

    - meta: flush_handlers
